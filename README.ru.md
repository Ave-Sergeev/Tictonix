<p align="center">
  <img src="./assets/logo.png" alt="Project Preview" style="width: 55%; height: auto; border-radius: 15px;">
</p>
<h1 align="center"> 
  Tictonix 
</h1>

### Описание

Данный крейт предоставляет функционал для работы с векторными представлениями слов (эмбеддингами) и позиционным кодированием.
Он предназначается для применения в NLP задачах, глубоком обучении, и ваших кастомных проектах.

Так же данный проект является вторым шагом (1-й шаг [токенизатор](https://github.com/Ave-Sergeev/Tokenomicon)) на пути к
собственной реализации `LLM` на архитектуре `Transformer`.

### Предоставляемый функционал:

- Структура Embeddings

1) Создание новой матрицы эмбеддингов различными методами, такими как: `Gaussian`, `Xavier`, `Uniform`.
2) Построение результирующей матрицы эмбеддингов для массива токенов (индексов), и получение конкретного эмбеддинга по токену (индексу).
3) Обновление (замена) эмбеддинга для конкретного токена (индекса).

- Структура PositionalEncoding

1) Создание новой матрицы позиционных кодировок различными методами, такими как: `Sinusoidal PE`, `Relative PE`, `Rotary PE`.
2) Применение позиционных кодировок к матрице эмбеддингов.
3) Возврат части матрицы позиционных кодировок для последовательности, и конкретной позиционной кодировки по ее позиции.

- Структура MatrixIO

1) Сохранение в файл, и получение из файла матрицы эмбеддингов. Доступны форматы .safetensors и .npy.

UPD: Важное уточнение. 
В данной реализации у матрицы эмбеддингов столбцы соответствуют токенам (каждый столбец - эмбеддинг для одного токена).

### Установка

Добавьте в ваш `Cargo.toml`:
```toml
[dependencies]
tictonix = "0.9.0"
```

### Использование

Смотреть [примеры использования](https://github.com/Ave-Sergeev/Tictonix/blob/main/example/src/main.rs) в коде.

### Документация

Смотреть [документацию](https://docs.rs/tictonix/0.9.0/tictonix/) на проект.

### Глоссарий:

- Токенизация — это процесс разбиения текста на отдельные элементы, называемые токенами.
  Токенами могут быть слова, символы, субслова или другие единицы, в зависимости от выбранного метода токенизации.
  Этот процесс является важным этапом предобработки текста для задач обработки естественного языка (NLP).
- LLM (large language models) — это большие языковые модели, основанные на архитектурах глубокого обучения (например,
  Transformer), которые обучаются на огромных объемах текстовых данных. Они предназначены для выполнения широкого
  спектра задач, связанных с обработкой естественного языка, таких как генерация текста, перевод, ответы на вопросы,
  классификация и другие. LLM способны обобщать знания и выполнять задачи, на которых они не были явно обучены
  (zero-shot или few-shot learning).
- Transformer — это архитектура нейронных сетей, предложенная в 2017 году, которая использует механизм внимания для
  обработки последовательностей данных, таких как текст. Основное преимущество Transformer
  заключается в его способности обрабатывать длинные последовательности и учитывать контекст независимо от расстояния
  между элементами последовательности. Эта архитектура лежит в основе большинства современных LLM (GPT, BERT и т.д.).
- Embedding — это числовое (векторное) представление текстовых данных (токенов, слов, фраз или предложений).
- Positional Encoding — это метод, используемый в архитектуре Transformer для передачи информации о порядке элементов в
  последовательности. Поскольку Transformer не имеет встроенной информации о порядке (в отличие от рекуррентных сетей),
  позиционное кодирование добавляет к эмбеддингам токенов специальные сигналы, зависящие от их позиции в
  последовательности. Это позволяет модели учитывать порядок слов или других элементов во входных данных.

### P.S.

Уважаемый!
Если вам что-то приглянулось в [данном проекте](https://github.com/Ave-Sergeev/Tictonix), сочли его полезным, или просто понравился код - не стесняйся поставить ⭐
звездочку в благодарность.
