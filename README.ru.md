<p align="center">
  <img src="./assets/logo.png" alt="Project Preview" style="width: 55%; height: auto; border-radius: 15px;">
</p>
<h1 align="center"> 
  Tictonix 
</h1>

### Описание

Данный крейт предоставляет функционал для работы с векторными представлениями слов (эмбеддингами) и позиционным кодированием.
Он предназначается для применения в NLP задачах, глубоком обучении, и ваших кастомных проектах.

Так же данный проект является вторым шагом (1-й шаг [токенизатор](https://github.com/Ave-Sergeev/Tokenomicon)) на пути к
собственной реализации `LLM` на архитектуре `Transformer`.

### Предоставляемый функционал:

#### Структура Embeddings

- Создание новой матрицы эмбеддингов различными методами, такими как: `Gaussian`, `Xavier`, `Uniform`.
- Построение результирующей матрицы эмбеддингов для массива токенов (индексов), и получение конкретного эмбеддинга по токену (индексу).
- Обновление (замена) эмбеддинга для конкретного токена (индекса).

#### Структура PositionalEncoding

- Создание новой матрицы позиционных кодировок различными методами, такими как: `Sinusoidal PE`, `Relative PE`, `Rotary PE`.
- Применение позиционных кодировок к матрице эмбеддингов.
- Возврат части матрицы позиционных кодировок для последовательности, и конкретной позиционной кодировки по ее позиции.

#### Структура MatrixIO

- Сохранение в файл, и получение из файла матрицы эмбеддингов. Доступны форматы `.safetensors` и `.npy`.

**UPD: Важные уточнения:**

- В данной реализации у матрицы эмбеддингов столбцы соответствуют токенам (каждый столбец - эмбеддинг для одного токена).
- Все вычисления выполняются исключительно на CPU (без использования GPU).

### Установка

Добавьте в ваш `Cargo.toml`:
```toml
[dependencies]
tictonix = "1.0.1"
```

### Логирование (Журналирование)

В проекте используется [crate log](https://github.com/rust-lang/log) предоставляющий фасад журналирования.

На данный момент логгирование реализовано только для IO операций (сохранение и загрузка файлов), с сообщением об успешном выполнении.

Вы можете использовать любую совместимую реализацию (например `env_logger`, `fern`, `simple_logger`, `tracing`, ...)
Для этого достаточно проинициализировать выбранный логгер в вашем приложении перед началом работы.

Пример инициализации с `env_logger`:
```Rust
fn main() {
    Builder::new()
        .filter_level(LevelFilter::Info)
        .filter_module("tictonix", LevelFilter::Info)
        .init();
    // Ваш код
}
```

### Использование

Смотреть [примеры использования](https://github.com/Ave-Sergeev/Tictonix/blob/main/example/src/main.rs) в коде.

### Документация

Смотреть [документацию](https://docs.rs/tictonix/1.0.1/tictonix/) на проект.

### Глоссарий:

- Токенизация — это процесс разбиения текста на отдельные элементы, называемые токенами.
  Токенами могут быть слова, символы, субслова или другие единицы, в зависимости от выбранного метода токенизации.
  Этот процесс является важным этапом предобработки текста для задач обработки естественного языка (NLP).
- LLM (large language models) — это большие языковые модели, основанные на архитектурах глубокого обучения (например,
  Transformer), которые обучаются на огромных объемах текстовых данных. Они предназначены для выполнения широкого
  спектра задач, связанных с обработкой естественного языка, таких как генерация текста, перевод, ответы на вопросы,
  классификация и другие. LLM способны обобщать знания и выполнять задачи, на которых они не были явно обучены
  (zero-shot или few-shot learning).
- Transformer — это архитектура нейронных сетей, предложенная в 2017 году, которая использует механизм внимания для
  обработки последовательностей данных, таких как текст. Основное преимущество Transformer
  заключается в его способности обрабатывать длинные последовательности и учитывать контекст независимо от расстояния
  между элементами последовательности. Эта архитектура лежит в основе большинства современных LLM (GPT, BERT и т.д.).
- Embedding — это числовое (векторное) представление текстовых данных (токенов, слов, фраз или предложений).
- Positional Encoding — это метод, используемый в архитектуре Transformer для передачи информации о порядке элементов в
  последовательности. Поскольку Transformer не имеет встроенной информации о порядке (в отличие от рекуррентных сетей),
  позиционное кодирование добавляет к эмбеддингам токенов специальные сигналы, зависящие от их позиции в
  последовательности. Это позволяет модели учитывать порядок слов или других элементов во входных данных.

### P.S.

Уважаемый!
Если вам что-то приглянулось в [данном проекте](https://github.com/Ave-Sergeev/Tictonix), сочли его полезным, или просто понравился код - не стесняйся поставить ⭐
звездочку в благодарность.
