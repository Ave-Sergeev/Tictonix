## Tictonix

---

### Описание

Данный крейт является вторым шагом (1-й шаг [токенизатор](https://github.com/Ave-Sergeev/Tokenomicon)) на пути к
собственной реализации `LLM` на архитектуре `Transformer`.
В нем содержатся операции, связанные с преобразованием токенов в эмбединги, кодированием их позиций.

### Предоставляемый функционал:

- Структура Embeddings

1) Создание новой матрицы эмбеддингов различными методами (`Gaussian`, `Xavier`, `Uniform`).
2) Преобразование вектора токенов в матрицу эмбеддингов.
3) Сохранение в файл (формат .safetensors), и получение из файла матрицы эмбеддингов.

- Структура PositionalEncoding

1) Создание новой матрицы позиционных кодировок различными методами, такими как:
   `Sinusoidal PE`, `Relative PE`, `Rotary PE`.
2) Применение позиционных кодировок к матрице эмбеддингов.
3) Возврат части матрицы позиционных кодировок для последовательности.

Крейт имеет следующие зависимости:

1) [rand](https://github.com/rust-random/rand) крейт для генерации псевдо-случайных значений.
2) [ndarray](https://github.com/rust-ndarray/ndarray) крейт (математический) для эффективной работы с матрицами.
3) [anyhow](https://github.com/dtolnay/anyhow) крейт для идиоматической обработки ошибок.
4) [approx](https://github.com/brendanzab/approx) крейт для работы с приближенными сравнениями чисел с плавающей точкой.
5) [bytemuck](https://github.com/Lokathor/bytemuck) крейт для преобразования простых типов данных.
6) [thiserror](https://github.com/dtolnay/thiserror) крейт для удобного вывода ошибок.
7) [safetensors](https://github.com/huggingface/safetensors) крейт для безопасного хранения тензоров.
8) ...

### Использование

Смотреть [пример](https://github.com/Ave-Sergeev/Tictonix/blob/main/example/src/main.rs) использования.

### Глоссарий:

- Токенизация — это процесс разбиения текста на отдельные элементы, называемые токенами.
  Токенами могут быть слова, символы, субслова или другие единицы, в зависимости от выбранного метода токенизации.
  Этот процесс является важным этапом предобработки текста для задач обработки естественного языка (NLP).
- LLM (large language models) — это большие языковые модели, основанные на архитектурах глубокого обучения (например,
  Transformer), которые обучаются на огромных объемах текстовых данных. Они предназначены для выполнения широкого
  спектра задач, связанных с обработкой естественного языка, таких как генерация текста, перевод, ответы на вопросы,
  классификация и другие. LLM способны обобщать знания и выполнять задачи, на которых они не были явно обучены
  (zero-shot или few-shot learning).
- Transformer — это архитектура нейронных сетей, предложенная в 2017 году, которая использует механизм внимания для
  обработки последовательностей данных, таких как текст. Основное преимущество Transformer
  заключается в его способности обрабатывать длинные последовательности и учитывать контекст независимо от расстояния
  между элементами последовательности. Эта архитектура лежит в основе большинства современных LLM (GPT, BERT и т.д.).
- Embedding — это числовое (векторное) представление текстовых данных (токенов, слов, фраз или предложений).
- Positional Encoding — это метод, используемый в архитектуре Transformer для передачи информации о порядке элементов в
  последовательности. Поскольку Transformer не имеет встроенной информации о порядке (в отличие от рекуррентных сетей),
  позиционное кодирование добавляет к эмбеддингам токенов специальные сигналы, зависящие от их позиции в
  последовательности. Это позволяет модели учитывать порядок слов или других элементов во входных данных.

### P.S.

Уважаемый!
Если вам что-то приглянулось в данном проекте, сочли его полезным, или просто понравился код - не стесняйся поставить ⭐
звездочку в благодарность.
