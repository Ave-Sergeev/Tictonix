## Tictonix

---

### Описание

Данный крейт предоставляет функционал для работы с векторными представлениями слов (эмбеддингами) и позиционным кодированием.
Он предназначается для применения в NLP задачах, глубоком обучении, и ваших кастомных проектах.

Так же данный проект является вторым шагом (1-й шаг [токенизатор](https://github.com/Ave-Sergeev/Tokenomicon)) на пути к
собственной реализации `LLM` на архитектуре `Transformer`.

### Предоставляемый функционал:

- Структура Embeddings

1) Создание новой матрицы эмбеддингов различными методами, такими как: `Gaussian`, `Xavier`, `Uniform`.
2) Построение результирующей матрицы эмбеддингов для массива токенов (индексов), и получение конкретного эмбеддинга по токену (индексу).
3) Обновление (замена) эмбеддинга для конкретного токена (индекса).

- Структура PositionalEncoding

1) Создание новой матрицы позиционных кодировок различными методами, такими как: `Sinusoidal PE`, `Relative PE`, `Rotary PE`.
2) Применение позиционных кодировок к матрице эмбеддингов.
3) Возврат части матрицы позиционных кодировок для последовательности, и конкретной позиционной кодировки по ее позиции.

- Структура MatrixIO

1) Сохранение в файл (формат .safetensors), и получение из файла матрицы эмбеддингов.

### Установка

Добавьте в ваш `Cargo.toml`:
```toml
[dependencies]
tictonix = "0.7.0"
```

### Использование

Смотреть [примеры использования](https://github.com/Ave-Sergeev/Tictonix/blob/main/example/src/main.rs) в коде.

### Документация

Смотреть [документацию](https://docs.rs/tictonix/0.7.0/tictonix/) на проект.

### Зависимости:

1) [rand](https://github.com/rust-random/rand) крейт для генерации псевдо-случайных значений.
2) [ndarray](https://github.com/rust-ndarray/ndarray) крейт (математический) для эффективной работы с матрицами.
3) [anyhow](https://github.com/dtolnay/anyhow) крейт для идиоматической обработки ошибок.
4) [approx](https://github.com/brendanzab/approx) крейт для работы с приближенными сравнениями чисел с плавающей точкой.
5) [bytemuck](https://github.com/Lokathor/bytemuck) крейт для преобразования простых типов данных.
6) [thiserror](https://github.com/dtolnay/thiserror) крейт для удобного вывода ошибок.
7) [safetensors](https://github.com/huggingface/safetensors) крейт для безопасного хранения тензоров.
8) ...

### Глоссарий:

- Токенизация — это процесс разбиения текста на отдельные элементы, называемые токенами.
  Токенами могут быть слова, символы, субслова или другие единицы, в зависимости от выбранного метода токенизации.
  Этот процесс является важным этапом предобработки текста для задач обработки естественного языка (NLP).
- LLM (large language models) — это большие языковые модели, основанные на архитектурах глубокого обучения (например,
  Transformer), которые обучаются на огромных объемах текстовых данных. Они предназначены для выполнения широкого
  спектра задач, связанных с обработкой естественного языка, таких как генерация текста, перевод, ответы на вопросы,
  классификация и другие. LLM способны обобщать знания и выполнять задачи, на которых они не были явно обучены
  (zero-shot или few-shot learning).
- Transformer — это архитектура нейронных сетей, предложенная в 2017 году, которая использует механизм внимания для
  обработки последовательностей данных, таких как текст. Основное преимущество Transformer
  заключается в его способности обрабатывать длинные последовательности и учитывать контекст независимо от расстояния
  между элементами последовательности. Эта архитектура лежит в основе большинства современных LLM (GPT, BERT и т.д.).
- Embedding — это числовое (векторное) представление текстовых данных (токенов, слов, фраз или предложений).
- Positional Encoding — это метод, используемый в архитектуре Transformer для передачи информации о порядке элементов в
  последовательности. Поскольку Transformer не имеет встроенной информации о порядке (в отличие от рекуррентных сетей),
  позиционное кодирование добавляет к эмбеддингам токенов специальные сигналы, зависящие от их позиции в
  последовательности. Это позволяет модели учитывать порядок слов или других элементов во входных данных.

### P.S.

Уважаемый!
Если вам что-то приглянулось в данном проекте, сочли его полезным, или просто понравился код - не стесняйся поставить ⭐
звездочку в благодарность.
